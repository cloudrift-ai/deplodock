{
  "task": {
    "recipe_dir": "experiments/Qwen3-Coder-30B-A3B-Instruct-AWQ/optimal_ctx_rtx5090",
    "variant": "rtx5090x1_cl40960",
    "gpu_name": "NVIDIA GeForce RTX 5090",
    "gpu_short": "rtx5090",
    "gpu_count": 1
  },
  "recipe": {
    "model": {
      "huggingface": "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ"
    },
    "engine": {
      "llm": {
        "context_length": 40960,
        "max_concurrent_requests": 4,
        "tensor_parallel_size": 1,
        "pipeline_parallel_size": 1,
        "gpu_memory_utilization": 0.9,
        "vllm": {
          "image": "vllm/vllm-openai:latest",
          "extra_args": ""
        },
        "sglang": null
      }
    },
    "benchmark": {
      "max_concurrency": 4,
      "num_prompts": 8,
      "random_input_len": 4000,
      "random_output_len": 4000
    },
    "deploy": {
      "gpu": "NVIDIA GeForce RTX 5090",
      "gpu_count": 1
    }
  },
  "metrics": {
    "successful_requests": 8,
    "failed_requests": 0,
    "max_request_concurrency": 4,
    "benchmark_duration_s": 57.73,
    "total_input_tokens": 32000,
    "total_generated_tokens": 32000,
    "request_throughput": 0.14,
    "output_token_throughput": 554.34,
    "peak_output_token_throughput": 656.0,
    "peak_concurrent_requests": 8.0,
    "total_token_throughput": 1108.68,
    "mean_ttft_ms": 657.57,
    "median_ttft_ms": 658.99,
    "p99_ttft_ms": 966.37,
    "mean_tpot_ms": 7.05,
    "median_tpot_ms": 7.05,
    "p99_tpot_ms": 7.36,
    "mean_itl_ms": 7.05,
    "median_itl_ms": 7.02,
    "p99_itl_ms": 8.08,
    "mean_e2el_ms": null,
    "median_e2el_ms": null,
    "p99_e2el_ms": null
  },
  "system": {
    "hostname": "riftvm",
    "os": "Ubuntu 24.04.1 LTS",
    "kernel": "6.8.0-51-generic",
    "cpu_model": "AMD EPYC 7702 64-Core Processor",
    "cpu_count": 7,
    "cpu_arch": "x86_64",
    "memory_total_gib": 49.0,
    "gpu_name": "NVIDIA GeForce RTX 5090",
    "gpu_memory_mib": 32607,
    "gpu_driver": "580.65.06",
    "cuda_version": "13.0",
    "gpu_count": 1,
    "docker_version": "28.5.1"
  },
  "compose": "services:\n\n  vllm_0:\n    image: vllm/vllm-openai:latest\n    container_name: vllm_0\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n    volumes:\n      - /hf_models:/hf_models\n    environment:\n      - HUGGING_FACE_HUB_TOKEN=***\n      - HF_HOME=/hf_models\n    ports:\n      - \"8000:8000\"\n    shm_size: '16gb'\n    ipc: host\n    command: >\n      --trust-remote-code\n      --gpu-memory-utilization=0.9\n      --host 0.0.0.0\n      --port 8000\n      --tensor-parallel-size 1\n      --pipeline-parallel-size 1\n      --model QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ\n      --served-model-name QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ\n      --max-model-len 40960\n      --max-num-seqs 4\n    healthcheck:\n      test: [\"CMD\", \"bash\", \"-c\", \"curl -f http://localhost:8000/health\"]\n      interval: 10s\n      timeout: 10s\n      retries: 180\n      start_period: 600s\n",
  "bench_command": "vllm bench serve\n    --model QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ\n    --max-concurrency 4\n    --num-prompts 8\n    --random-input-len 4000\n    --random-output-len 4000\n    --base-url http://localhost:8000"
}
