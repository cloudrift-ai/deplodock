{
  "task": {
    "recipe_dir": "experiments/Qwen3-Coder-Next-FP8/vllm_vs_sglang_full_ctx_optimal_mcr_pro6000",
    "variant": "pro6000",
    "gpu_name": "NVIDIA RTX PRO 6000 Workstation Edition",
    "gpu_short": "pro6000",
    "gpu_count": 1
  },
  "recipe": {
    "model": {
      "huggingface": "Qwen/Qwen3-Coder-Next-FP8"
    },
    "engine": {
      "llm": {
        "context_length": 131072,
        "max_concurrent_requests": 4,
        "tensor_parallel_size": 1,
        "pipeline_parallel_size": 1,
        "gpu_memory_utilization": 0.9,
        "vllm": {
          "image": "vllm/vllm-openai:latest",
          "extra_args": ""
        },
        "sglang": null
      }
    },
    "benchmark": {
      "max_concurrency": 8,
      "num_prompts": 8,
      "random_input_len": 8000,
      "random_output_len": 8000
    },
    "deploy": {
      "gpu": "NVIDIA RTX PRO 6000 Workstation Edition",
      "gpu_count": 1
    }
  },
  "metrics": {
    "successful_requests": 8,
    "failed_requests": 0,
    "max_request_concurrency": 8,
    "benchmark_duration_s": 196.14,
    "total_input_tokens": 64000,
    "total_generated_tokens": 64000,
    "request_throughput": 0.04,
    "output_token_throughput": 326.3,
    "peak_output_token_throughput": 388.0,
    "peak_concurrent_requests": 8.0,
    "total_token_throughput": 652.6,
    "mean_ttft_ms": 64404.04,
    "median_ttft_ms": 64632.18,
    "p99_ttft_ms": 111398.13,
    "mean_tpot_ms": 11.13,
    "median_tpot_ms": 11.11,
    "p99_tpot_ms": 11.7,
    "mean_itl_ms": 11.13,
    "median_itl_ms": 11.14,
    "p99_itl_ms": 13.75,
    "mean_e2el_ms": null,
    "median_e2el_ms": null,
    "p99_e2el_ms": null
  },
  "system": {
    "hostname": "riftvm",
    "os": "Ubuntu 24.04.1 LTS",
    "kernel": "6.8.0-51-generic",
    "cpu_model": "AMD EPYC 9474F 48-Core Processor",
    "cpu_count": 11,
    "cpu_arch": "x86_64",
    "memory_total_gib": 98.0,
    "gpu_name": "NVIDIA RTX PRO 6000 Blackwell Workstation Edition",
    "gpu_memory_mib": 97887,
    "gpu_driver": "580.65.06",
    "cuda_version": "13.0",
    "gpu_count": 1,
    "docker_version": "28.5.1"
  },
  "compose": "services:\n\n  vllm_0:\n    image: vllm/vllm-openai:latest\n    container_name: vllm_0\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n    volumes:\n      - /hf_models:/hf_models\n    environment:\n      - HUGGING_FACE_HUB_TOKEN=***\n      - HF_HOME=/hf_models\n    ports:\n      - \"8000:8000\"\n    shm_size: '16gb'\n    ipc: host\n    command: >\n      --trust-remote-code\n      --gpu-memory-utilization=0.9\n      --host 0.0.0.0\n      --port 8000\n      --tensor-parallel-size 1\n      --pipeline-parallel-size 1\n      --model Qwen/Qwen3-Coder-Next-FP8\n      --served-model-name Qwen/Qwen3-Coder-Next-FP8\n      --max-model-len 131072\n      --max-num-seqs 4\n    healthcheck:\n      test: [\"CMD\", \"bash\", \"-c\", \"curl -f http://localhost:8000/health\"]\n      interval: 10s\n      timeout: 10s\n      retries: 180\n      start_period: 600s\n",
  "bench_command": "vllm bench serve\n    --model Qwen/Qwen3-Coder-Next-FP8\n    --max-concurrency 8\n    --num-prompts 8\n    --random-input-len 8000\n    --random-output-len 8000\n    --base-url http://localhost:8000"
}
