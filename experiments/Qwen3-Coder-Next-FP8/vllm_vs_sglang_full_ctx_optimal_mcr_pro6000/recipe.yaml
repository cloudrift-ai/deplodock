model:
  huggingface: "Qwen/Qwen3-Coder-Next-FP8"

engine:
  llm:
    tensor_parallel_size: 1
    pipeline_parallel_size: 1
    gpu_memory_utilization: 0.9
    context_length: 131072
    max_concurrent_requests: 32

benchmark:
  max_concurrency: 16
  num_prompts: 64
  random_input_len: 4000
  random_output_len: 4000

matrices:
  - deploy.gpu: "NVIDIA RTX PRO 6000 Workstation Edition"
    deploy.gpu_count: 1
    engine.llm.vllm.image: "vllm/vllm-openai:latest"
  - deploy.gpu: "NVIDIA RTX PRO 6000 Workstation Edition"
    deploy.gpu_count: 1
    engine.llm.sglang.image: "lmsysorg/sglang:latest"
