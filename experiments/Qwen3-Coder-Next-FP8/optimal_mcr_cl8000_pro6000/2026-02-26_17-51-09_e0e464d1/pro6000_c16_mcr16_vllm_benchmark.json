{
  "task": {
    "recipe_dir": "experiments/Qwen3-Coder-Next-FP8/optimal_mcr_cl8000_pro6000",
    "variant": "pro6000_c16_mcr16",
    "gpu_name": "NVIDIA RTX PRO 6000 Workstation Edition",
    "gpu_short": "pro6000",
    "gpu_count": 1
  },
  "recipe": {
    "model": {
      "huggingface": "Qwen/Qwen3-Coder-Next-FP8"
    },
    "engine": {
      "llm": {
        "context_length": 16384,
        "max_concurrent_requests": 16,
        "tensor_parallel_size": 1,
        "pipeline_parallel_size": 1,
        "gpu_memory_utilization": 0.9,
        "vllm": {
          "image": "vllm/vllm-openai:latest",
          "extra_args": ""
        },
        "sglang": null
      }
    },
    "benchmark": {
      "max_concurrency": 16,
      "num_prompts": 16,
      "random_input_len": 8000,
      "random_output_len": 8000
    },
    "deploy": {
      "gpu": "NVIDIA RTX PRO 6000 Workstation Edition",
      "gpu_count": 1
    }
  },
  "metrics": {
    "successful_requests": 16,
    "failed_requests": 0,
    "max_request_concurrency": 16,
    "benchmark_duration_s": 165.03,
    "total_input_tokens": 128000,
    "total_generated_tokens": 128000,
    "request_throughput": 0.1,
    "output_token_throughput": 775.62,
    "peak_output_token_throughput": 928.0,
    "peak_concurrent_requests": 16.0,
    "total_token_throughput": 1551.23,
    "mean_ttft_ms": 18243.18,
    "median_ttft_ms": 18250.84,
    "p99_ttft_ms": 19961.07,
    "mean_tpot_ms": 18.34,
    "median_tpot_ms": 18.34,
    "p99_tpot_ms": 18.55,
    "mean_itl_ms": 18.34,
    "median_itl_ms": 18.17,
    "p99_itl_ms": 19.82,
    "mean_e2el_ms": null,
    "median_e2el_ms": null,
    "p99_e2el_ms": null
  },
  "system": {
    "hostname": "riftvm",
    "os": "Ubuntu 24.04.1 LTS",
    "kernel": "6.8.0-51-generic",
    "cpu_model": "AMD EPYC 9474F 48-Core Processor",
    "cpu_count": 11,
    "cpu_arch": "x86_64",
    "memory_total_gib": 98.0,
    "gpu_name": "NVIDIA RTX PRO 6000 Blackwell Workstation Edition",
    "gpu_memory_mib": 97887,
    "gpu_driver": "580.65.06",
    "cuda_version": "13.0",
    "gpu_count": 1,
    "docker_version": "28.5.1"
  },
  "compose": "services:\n\n  vllm_0:\n    image: vllm/vllm-openai:latest\n    container_name: vllm_0\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n    volumes:\n      - /hf_models:/hf_models\n    environment:\n      - HUGGING_FACE_HUB_TOKEN=***\n      - HF_HOME=/hf_models\n    ports:\n      - \"8000:8000\"\n    shm_size: '16gb'\n    ipc: host\n    command: >\n      --trust-remote-code\n      --gpu-memory-utilization=0.9\n      --host 0.0.0.0\n      --port 8000\n      --tensor-parallel-size 1\n      --pipeline-parallel-size 1\n      --model Qwen/Qwen3-Coder-Next-FP8\n      --served-model-name Qwen/Qwen3-Coder-Next-FP8\n      --max-model-len 16384\n      --max-num-seqs 16\n    healthcheck:\n      test: [\"CMD\", \"bash\", \"-c\", \"curl -f http://localhost:8000/health\"]\n      interval: 10s\n      timeout: 10s\n      retries: 180\n      start_period: 600s\n",
  "bench_command": "vllm bench serve\n    --model Qwen/Qwen3-Coder-Next-FP8\n    --max-concurrency 16\n    --num-prompts 16\n    --random-input-len 8000\n    --random-output-len 8000\n    --base-url http://localhost:8000"
}
