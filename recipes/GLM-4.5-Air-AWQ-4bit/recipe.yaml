model:
  huggingface: "cpatonn/GLM-4.5-Air-AWQ-4bit"

engine:
  llm:
    tensor_parallel_size: 1
    pipeline_parallel_size: 1
    gpu_memory_utilization: 0.9
    context_length: 16384
    max_concurrent_requests: 256
    vllm:
      image: "vllm/vllm-openai:latest"
      extra_args: "--tool-call-parser glm45 --reasoning-parser glm45 --kv-cache-dtype fp8"

benchmark:
  max_concurrency: 128
  num_prompts: 256
  random_input_len: 8000
  random_output_len: 8000

matrices:
  - deploy.gpu: "NVIDIA B200"
    deploy.gpu_count: 1
    benchmark.max_concurrency: 256
    benchmark.num_prompts: 512
  - deploy.gpu: "NVIDIA H200 141GB"
    deploy.gpu_count: 1
    benchmark.max_concurrency: 256
    benchmark.num_prompts: 512
  - deploy.gpu: "NVIDIA H100 80GB"
    deploy.gpu_count: 1
    engine.llm.vllm.extra_args: "--tool-call-parser glm45 --reasoning-parser glm45 --enable-expert-parallel --kv-cache-dtype fp8"
  - deploy.gpu: "NVIDIA RTX PRO 6000 Workstation Edition"
    deploy.gpu_count: 1
    engine.llm.vllm.extra_args: "--tool-call-parser glm45 --reasoning-parser glm45 --enable-expert-parallel --kv-cache-dtype fp8"
  - deploy.gpu: "NVIDIA GeForce RTX 5090"
    deploy.gpu_count: 4
    engine.llm.pipeline_parallel_size: 4
    engine.llm.vllm.extra_args: "--tool-call-parser glm45 --reasoning-parser glm45 --enable-expert-parallel --kv-cache-dtype fp8"
  - deploy.gpu: "NVIDIA GeForce RTX 4090"
    deploy.gpu_count: 4
    engine.llm.pipeline_parallel_size: 4
    engine.llm.vllm.extra_args: "--dtype float16 --tool-call-parser glm45 --reasoning-parser glm45 --enable-expert-parallel"
  - deploy.gpu: "NVIDIA L40S"
    deploy.gpu_count: 2
    engine.llm.pipeline_parallel_size: 2
    engine.llm.vllm.extra_args: "--tool-call-parser glm45 --reasoning-parser glm45 --enable-expert-parallel --kv-cache-dtype fp8"
